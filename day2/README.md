# homework1

## 実装

### delete機能を実装
- **先頭要素の削除**  
  `self.buckets[bucket_index] = item.next`でバケットの先頭を更新

- **中間・末尾要素の削除**  
  `prev.next = item.next`で前の要素と次の要素を接続

- **要素が存在しない場合**  
  `False`を返して削除失敗を通知



### 再ハッシュ関数を追加

- **拡大条件**：使用率が70%以上の場合、サイズを2倍+1に拡大
- **縮小条件**：使用率が30%未満の場合、サイズを半分に縮小（最小97は維持）
- **無限ループ防止**：リハッシュ中は再帰的リハッシュにならないよう実装

疑問：70%、30%のところはどこで計算実行するのが一番良いか？
### hash値の計算を、衝突が少なくなるように改善
素数を用いて、桁数に応じて重み付けすればいいのではないか  
↓  
衝突は減るが、値と素数によってはアナグラム問題が解決されない  
↓  
位置による重み付け+ローリングハッシュを採用  
- 31の部分は他の素数でも良いのかと思っていたが、31が経験則的に良いらしい    
- 0xFFFFFFFFで32ビット制限をかけることで計算効率およびメモリ効率を改善  


## 再ハッシュの効果

| イテレーション | 再ハッシュなし (秒) | 再ハッシュあり (秒) | 改善率 |
|:------------:|:------------------:|:-----------------:|:-----:|
| 0 | 0.189091 | 0.165683 | 1.14x |
| 10 | 2.751768 | 0.097865 | 28.1x |
| 20 | 8.971665 | 0.152655 | 58.8x |
| 30 | 14.808546 | 0.099297 | 149.1x |
| 40 | 20.026890 | 0.098274 | 203.8x |
| 50 | 19.243019 | 0.100279 | 191.9x |
| 60 | - | 0.101702 | - |
| 70 | - | 0.097911 | - |
| 80 | - | 0.100595 | - |
| 90 | - | 0.101465 | - |
| 99 | - | 0.472530 | - |

## 性能

|  | 再ハッシュなし | 再ハッシュあり |
|:-----|:-------------|:-------------|
| **初期性能** | 約0.19秒 | 約0.17秒 |
| **50回目の性能** | 約19秒 | 約0.1秒 |
| **平均計算量** | O(n) | O(1) |
| **最悪計算量** | O(n) | O(n) |
| **空間計算量** | O(n) | O(n) |







# homework2
## 実際の大規模データベースにおいてハッシュテーブルより木構造が適している理由
### ハッシュテーブルだと
- 大規模だと異なるキーが同じハッシュ値をもつ可能性が高く、その分衝突が起きやすい
- 再ハッシュが必要になった際に、テーブルの全要素を入れ替える必要がある
- ハッシュ値で管理されているため空の領域が存在する可能性が高く、メモリ効率が悪い
- ハッシュ値の順序が本質的な意味を持たないため、部分的な操作が困難

### 木構造だと
- 必要な要素の分だけメモリを消費するため、メモリ効率が良い
- 構造的に順序関係を反映されているため、部分的な操作が効率的


# homework3
キャッシュの計算がO(1)でできる、もっとも直近にアクセスされた上位 X 個の <URL, Web ページ> の組が保存できるデータ構造を考える

ハッシュテーブルと連結リストを組み合わせると良いのでは？

ハッシュテーブル：検索がO(1)  
連結リスト：順序を格納


# homework4